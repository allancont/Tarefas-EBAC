{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47ec9e2",
   "metadata": {},
   "source": [
    "### 1. Cite 5 diferenças entre o AdaBoost e o GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c40d4c3",
   "metadata": {},
   "source": [
    "|AdaBoost| GBM|\n",
    "|-------------|---------|\n",
    "|Floresta de _\"Stumps\"_|Floresta de árvores |\n",
    "|Primeiro passo é um _\"Stumps\"_ |Primeiro passo é a média|\n",
    "|Respostas têm pesos diferentes|Todas respostas têm um multiplicador em comum chamado _learning rate_|\n",
    "|Utiliza várias funções de perda tornando o algorítmo sensível aos outliers |Qualquer função de perda diferenciável pode ser utilizada, o que torna o algorítmo mais robusto para outliers|\n",
    "|Cada classificador tem diferentes pesos atribuídos à previsão final com base em seu desempenho |Todos os classificadores são pesados igualmente e sua capacidade preditiva é restrita com taxa de aprendizado para aumentar a precisão |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f647d49",
   "metadata": {},
   "source": [
    "### 2. Acesse o link Scikit-learn – GBM, leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "100e6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "607b5a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(\n",
    "    n_estimators=100, \n",
    "    learning_rate=1.0,\n",
    "    max_depth=1, \n",
    "    random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff23fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values...: [-1.  1.]\n",
      "Frequency Values: [6068 5932]\n",
      "Total de valores:(12000,)\n"
     ]
    }
   ],
   "source": [
    "ini_array = y\n",
    "  \n",
    "unique, frequency = np.unique(ini_array,  \n",
    "                              return_counts = True) \n",
    "print(\"Unique Values...:\",  \n",
    "      unique) \n",
    "print(\"Frequency Values:\", \n",
    "      frequency)\n",
    "print(f'Total de valores:{y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0ce481",
   "metadata": {},
   "source": [
    "### 3. Cite 5 Hyperparametros importantes no GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735fbc26",
   "metadata": {},
   "source": [
    "- **loss**: Função de perda a ser otimizada, podendo ser: **log_loss** quando o desvio for binomial e multidirecional ou **exponencial**;\n",
    "- **learning_rate**: A taxa de aprendizado diminui a contribuição de cada árvore. Há uma compensação entre learning_rate e n_estimators, devendo os valores estar no intervalo de 0.0 a infinito;\n",
    "- **min_samples_split**: O número mínimo de amostras necessárias para dividir um nó interno;\n",
    "- **min_samples_leaf**: O número mínimo de amostras necessárias para estar em um nó folha;\n",
    "- **max_depth**: Profundidade máxima dos estimadores de regressão individuais;\n",
    "- **max_features**: O número de recursos a serem considerados ao procurar a melhor divisão;\n",
    "- **criterion**: A função para medir a qualidade de uma divisão. Os critérios suportados são **friedman_mse** para o erro quadrático médio com pontuação de melhoria de Friedman, **squared_error** para o erro quadrático médio. O valor padrão de **friedman_mse** geralmente é o melhor, pois pode fornecer uma aproximação melhor em alguns casos;\n",
    "- **subsample**: Fração de amostras a ser usadas para ajustar os _base learners_ individuais. Se este valor for menor que 1.0 o resultado será igual ao **Stochastic Gradient Boosting**. O **subsample** interage com o parâmetro **n_estimators**. Ao escolher valores menores que 1.0 haverá uma redução da variação e um aumento do viés. Os valores precisam estar entre 0.0 e 1.0;\n",
    "- **n_estimators**: O número de estágios de **boosting** a serem executados. O reforço de gradiente é bastante robusto ao ajuste excessivo, portanto, um grande número geralmente resulta em melhor desempenho. Os valores devem estar no intervalo entre 1 e inf.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eda32e",
   "metadata": {},
   "source": [
    "### 4. (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ec392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import make_scorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef8a7db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19min 23s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 'friedman_mse',\n",
       " 'learning_rate': 0.2,\n",
       " 'loss': 'deviance',\n",
       " 'max_depth': 8,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 31,\n",
       " 'n_estimators': 5,\n",
       " 'subsample': 0.85}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Parâmetros de Scoring: \n",
    "scoring = {'accuracy':  make_scorer(accuracy_score),\n",
    "           'precision': make_scorer(precision_score),\n",
    "           'recall':    make_scorer(recall_score)}\n",
    "\n",
    "# Parâmetros para amostras\n",
    "parameters = {\n",
    "    \"loss\":[\"deviance\"],\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],#list(np.arange (0.1, 3, 0.5)),\n",
    "    \"min_samples_split\": np.arange(1, 55, 5),\n",
    "    \"min_samples_leaf\": np.arange(1, 55, 5),\n",
    "    \"max_depth\":[3,5,8],\n",
    "    \"max_features\":[\"log2\",\"sqrt\"],\n",
    "    \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "    \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "    \"n_estimators\":[5]\n",
    "    }\n",
    "# Usando a função de scoring no GridSearchCV\n",
    "clf = GridSearchCV(GradientBoostingClassifier(), parameters,scoring=scoring,refit=False,cv=2, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Convertendo clf.cv_results em um dataframe\n",
    "df=pd.DataFrame.from_dict(clf.cv_results_)\n",
    "#here Possible inputs for cross validation is cv=2, there two split split0 and split1\n",
    "df[['split0_test_accuracy','split1_test_accuracy','split0_test_precision','split1_test_precision','split0_test_recall','split1_test_recall']]\n",
    "\n",
    "# Encontrando o melhor parâmetro com base no accuracy_score\n",
    "# Obtendo a média do accuracy_score\n",
    "df['accuracy_score']=(df['split0_test_accuracy']+df['split1_test_accuracy'])/2\n",
    "df.to_excel('criterios.xlsx', index=False)\n",
    "parametros = df.loc[df['accuracy_score'].idxmax()]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9cf4ba65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7841"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aplicando na base teste\n",
    "clf = GradientBoostingClassifier(**parametros).fit(X_train,y_train.ravel())\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d134ac8b",
   "metadata": {},
   "source": [
    "### 5. Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f5a650",
   "metadata": {},
   "source": [
    "A cada iteração uma subamostra é extraída aleatoriamente sem reposição do conjunto completo de dados de treinamento. Essa subamostra é então usada em vez de uma amostra completa para ajustar os _base learner_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
